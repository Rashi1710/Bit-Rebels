# Multi-Class Classification for Ambiguity of Question


!pip install transformers
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import tensorflow as tf
from transformers import BertTokenizer
from google.colab import files
uploaded = files.upload()
df_train = pd.read_csv('train.tsv.txt',sep='\t')
df_train.head()
df_train.shape
df_train.isnull().sum()
uploaded = files.upload()
df_test = pd.read_csv('dev.tsv',sep='\t')
df_train = df_train.dropna()
df_test = df_test.dropna()
df_train.head()
X = df_train['initial_request']
y = df_train['clarification_need']
y.value_counts()
## initialize tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
token = tokenizer.encode_plus(
    X.iloc[0], 
    max_length=256, #half the original length,just for less computation cost
    truncation=True, 
    padding='max_length', 
    add_special_tokens=True, #[cls] [sep] [NER]
    return_tensors='tf'
)
token.input_ids
# initialize empty zero arrays
X_input_ids = np.zeros((len(df_train), 256))
X_attn_masks = np.zeros((len(df_train), 256))
def generate_training_data(df, ids, masks, tokenizer):
    for i, text in tqdm(enumerate(X)):
        tokenized_text = tokenizer.encode_plus(
            text,
            max_length=256, 
            truncation=True, 
            padding='max_length', 
            add_special_tokens=True,
            return_tensors='tf'
        )
        # assign tokenized outputs to respective rows in numpy arrays
        ids[i, :] = tokenized_text.input_ids
        masks[i, :] = tokenized_text.attention_mask
    return ids, masks
X_input_ids, X_attn_masks = generate_training_data(df_train, X_input_ids, X_attn_masks, tokenizer)
num_samples = len(X);
# first extract review column
arr = y.values

# we then initialize the zero array
labels = np.zeros((num_samples, arr.max()+1))

# set relevant index for each row to 1 (one-hot encode)
labels[np.arange(num_samples), arr] = 1
#labels = np.zeros((len(df), 5))
labels.shape
labels[np.arange(len(df_train)), df_train['clarification_need'].values] = 1 # one-hot encoded target tensor
labels
# create the dataset object
# creating a data pipeline using tensorflow dataset utility, creates batches of data for easy loading...
dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))
dataset.take(1) # one sample data
def AmbigousDatasetMapFunction(input_ids, attn_masks, labels):
  # we convert our three-item tuple into a two-item tuple where the input item is a dictionary
    return {
        'input_ids': input_ids,
        'attention_mask': attn_masks
    }, labels
# then we use the dataset map method to apply this transformation
dataset = dataset.map(AmbigousDatasetMapFunction) # converting to required format for tensorflow dataset   
dataset.take(1)
# we will split into batches of 16
# shuffle and batch - dropping any remaining samples that don't cleanly fit into a batch of 16
dataset = dataset.shuffle(10000).batch(16, drop_remainder=True) # batch size, drop any left out tensor
dataset.take(1)
# set split size (90% training data) and calculate training set size
p = 0.8
train_size = int((len(df_train)//16)*p) # for each 16 batch of data we will have len(df)//16 samples, take 80% of that for train.
train_size

train_dataset = dataset.take(train_size) #90% train_dataset
val_dataset = dataset.skip(train_size) #10% val dataset
# Model

from transformers import TFBertModel
#Intialization of pre-trained Bert Model
model = TFBertModel.from_pretrained('bert-base-cased') # bert base model with pretrained weights
# defining 2 input layers for input_ids and attn_masks
input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')
attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')

bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)
intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)
output_layer = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes

Ambiguity_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)
Ambiguity_model.summary()
#optimization,Loss-Function and Accuracy model-Training Model Parameters
optim = tf.keras.optimizers.legacy.Adam(learning_rate=1e-5, decay=1e-6)
loss_func = tf.keras.losses.CategoricalCrossentropy()
acc = tf.keras.metrics.CategoricalAccuracy('accuracy')
Ambiguity_model.compile(optimizer=optim, loss=loss_func, metrics=[acc])
hist = Ambiguity_model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=2
)
Ambiguity_model.save('Ambiguity_model')
Prediction

Ambiguity_model = tf.keras.models.load_model('Ambiguity_model')

tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

def prepare_data(input_text, tokenizer):
    token = tokenizer.encode_plus(
        input_text,
        max_length=256, 
        truncation=True, 
        padding='max_length', 
        add_special_tokens=True,
        return_tensors='tf'
    )
    return {
        'input_ids': tf.cast(token.input_ids, tf.float64),
        'attention_mask': tf.cast(token.attention_mask, tf.float64)
    }

def make_prediction(model, processed_data, classes=['0', '1', '2', '3', '4']):
    probs = model.predict(processed_data)[0]
    return classes[np.argmax(probs)]
input_text = input('Enter Text here: ')
processed_data = prepare_data(input_text, tokenizer)
result = make_prediction(Ambiguity_model, processed_data=processed_data)
print(f"Predicted result: {result}")
